# Machine Learning with Python
## Task 1
见`output`和`script`文件夹
## Task 2
1）通常情况下，随着基分类器的增加，随机森林通常会收敛到更低的泛化误差，但是这种趋势并不是绝对的。当基分类器的数量足够多时，随机森林的泛化误差会趋于稳定。再增加基分类器的数目，效果基本不会提升，只会使得训练时间变长，因此，一般情况下，只要选择一个足够大的基分类器数目即可（如1000）。

2）Bagging是并行集成学习方法最著名的代表，他是基于bootstrap sampling做的有放回抽样，多次抽样后组成多组训练集，来训练多个模型。RandomForest就是基于Bagging做了一个扩展：随机选择属性（特征）。

out-of-bag (oob) error是 “包外误差”的意思。它指的是，我们在从x_data中进行多次有放回的采样，能构造出多个训练集。根据bootstrap sampling的特点，我们可以知道，在训练RF的过程中，一定会有约36%的样本永远不会被采样到。注意，这里说的“约36%的样本永远不会被采样到”，并不是针对第k棵树来说的，是针对所有树来说，36%的样本永远不会在任何一棵树的训练集中出现过。那这36%的样本，就是out-of-bag (oob) data，包外数据。用这个包外数据来做evaluation，就相当于用测试集来做evaluation。所以RF不需要再用测试集来做evaluation了。